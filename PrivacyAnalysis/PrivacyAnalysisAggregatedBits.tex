\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{url}
\usepackage{diagbox}
\usepackage{todonotes}

\newcommand{\VTNote}[1]{\todo{VT: #1}}
\newcommand{\prob}{\mathit{Pr}}
\newcommand{\U}{\mathcal{U}}

\begin{document}
	
	\title{Privacy of aggregated up- and down- votes for Right To Ask}

\section{Introduction}
This analysis is complementary to the analysis of \cite{kiayias2022privacy}, in that it looks at the privacy implications of revealing aggregated data, while that work examines sharing the trust assumptions around various participants so that the decryption capability is not held by a fixed set of parties.

We consider the privacy model appropriate for aggregated bits. In our setting, these bits are up-votes (1) and down-votes (0) in Right To Ask, which are homomorphically aggregated to produce totals without revealing any other information about individual votes.

Totals can of course reveal information, with certainty if they are unanimous and in a probabilistic way even if they are not. This report considers a series of increasingly complex models of vote privacy in this setting.

\subsection{What are we trying to achieve? Differences between a standard election privacy model and the privacy model of Right To Ask}

The privacy model for traditional first-past-the-post elections is simple: an election authority traditionally publishes exact aggregates, with coercion-resistance as a secondary goal. In principle, it might be possible to prove who won without revealing exact tallies; in practice as far as we know this is never done. In practice sometimes the level of aggregation is varied for privacy reasons (for example, sometimes aggregate tallies may not be revealed at the level of individual polling places or by method of voting, though often they are). The requirement for exact answers generally precludes differentially private solutions.

This means that there is some coercion risk, particularly to distinguished groups and particularly in the case of anonymity or near-anonymity. 

Right To Ask has a significantly different privacy model: there are many questions to be voted on, and people's contributions are probably highly correlated---they will tend to up-vote similar questions, and down-vote similar questions, if they see them. However, the system is less dependent on exact answers, and the consequences of individual privacy breach may be less serious, than in government elections.

In summary, the Right To Ask privacy model has the following characteristics. 
\begin{itemize}
	\item Each up- or down-vote is a public ciphertext. Thus the system reveals who responded to which questions.
	\item Aggregated tallies are revealed.
	\item Slight perturbations of aggregated tallies are acceptable.
	\item People respond to multiple questions, in a way that may be highly correlated.
\end{itemize}


\paragraph{Notation} Assume fixed batches of size $B$. A ballot can only ever appear in a single batch, and once counted will not be counted again as part of any future batch. 

Let $V = \{ v_1, v_2,\ldots,v_B\}$ be the set of votes.

\section{Unanimous tallies}

\subsection{A very simple random model}
First assume that each vote is chosen randomly and independently, with a probability $p$ of being 1 (and $1-p$ of being 0). This is obviously an oversimplified model to try out some ideas and get some best-case results.
Let $\U$ be the event that all votes are unanimous, either all ones or all zeros.

$$
\begin{array}{rcl}
	\prob_{p,B}(\U) & = & p^{B} + (1-p)^B \\
\end{array}
$$

This is minimised at $1/2^{B-1}$ when $p=1/2$, but rises quickly for uneven bit distributions. Some example values for $B=20$ are shown below.

$$
\begin{array}{cccc}
p & 0.5 & 0.7 & 0.9 \\
\prob_{p,20}(\U) & 2 \times 10^{-6} & 8 \times 10^{-4} & 0.12
\end{array}
$$

If an attacker controls $\alpha$ fraction of participants, and hence knows their votes, then in an average batch in which $\alpha B$ votes are exposed, the probability of the rest being unanimous is

$$
\begin{array}{rcl}
\prob_{p,(1-\alpha)B}(\U) & = & p^{(1-\alpha)B} + (1-p)^(1-\alpha)B \\
\end{array}
$$

Some example values of $\prob_{p,\alpha \times 20}(\U)$ are shown below.


\begin{tabular}{lccc}
\backslashbox{$\alpha$}{$p$}  & 0.5 & 0.7 & 0.9 \\
%\alpha & & & & \\
0 & $2 \times 10^{-6}$ &$ 8 \times 10^{-4}$ & 0.12 \\
0.05 & $2 \times 10^{-6}$ & $1 \times 10^{-3}$& 0.13 \\
0.1  & $8 \times 10^{-6}$ & $2 \times 10^{-3}$ & 0.15 \\
0.2  & $3 \times 10^{-5}$ & $ 3 \times 10^{-3}$ & 0.18  \\
\end{tabular}

The probability of being in at least one unanimous set after participating in $c$ batches can be easily computed.

\subsection{Adding randomness}
Suppose we add $r$ random bits (independently and uniformly chosen) into each batch before aggregation, then subtract $r/2$ from the resulting tallies. This does not affect the average tally, but lowers the probability of unanimity.

$$
\prob_{p,B}(\U | \text{$r$ random extra bits}) = \prob_{p,B+r}(\U)
$$

In the next section we will discuss how to add encrypted randomness in a verifiably fair way.

\section{Differentially private aggregates}
Differential Privacy provides a quantifiable and bounded privacy cost of a response to a query on a dataset. It achieves this by establishing a bound on the degree of distinguishability between two data sets, one containing an individual's record and one not. This degree of distinguishability is defined by the value of $\epsilon$. As a result, the impact of the inclusion of any one individual is bounded by $\epsilon$. A higher value of $\epsilon$ would allow a greater degree of distinguishability and therefore a larger amount of information about any one individual to be included within the response. Conversely, a lower value of $\epsilon$ would reduce the amount of information about any one individual included in the response. Ideally an $\epsilon$ of between 0 and 1 would be selected\footnote{$\epsilon$ should be greater than 0 since an $\epsilon$ of 0 would indicate that each individual has no impact on the response and therefore the response is generated independently of the underlying data and is therefore of no use.} to provide a reasonably level of privacy protection. Higher values of between 1 and 3 could be acceptable in some circumstances, but are not applicable to this scenario. 

To meet the bound set by $\epsilon$ random noise is added to the data, in our case counts, in order to provide the desired level of indistinguishability between the dataset containing a voters vote and the dataset not containing it. If this is achieved the voter would effectively have plausible deniability about how they had voted, since the inclusion or exclusion of the voter from that count would not have impacted on the result sufficiently to determine how they had voted. The random noise added to the response is commonly generated from the Laplace distribution centred on zero with a scale factor of the sensitivity divided by the value of $\epsilon$.

Whilst we typically discuss Differential Privacy in terms of responses to queries, a single release, for example a table of counts, can be considered to be the response to the single query used to generate the table of counts. As an additional note, responses to multiple queries of the same data would cause an accumulation in the $\epsilon$ values and therefore the total privacy cost. Typically a total $\epsilon$ would be defined and if multiple queries are to be supported that $\epsilon$ value would be subdivided between the various queries. Once the maximum $\epsilon$ has been reached no further queries would be permitted. The exact method for accumulation can vary and is beyond the scope of this paper, not least because, as we shall discuss below, we consider the individual questions to be independent and therefore do not accumulate privacy costs across them. We discuss whether the assumption the questions are independent in more detail below. 

In our context of counts of votes related to a specific question, each count can be considered to be an intermediate count consisting of unique individual's vote that are non-overlapping with previous intermediate counts. This is based on the assumption that individual's have only one account and are limited to either voting up, dismissing, or ignoring each question. As such, an individual will only ever have a single ballot entry for each question. This is important as it allows us to treat each intermediate count as a single response consisting of data from $n$ individuals, where $n$ is the size of the batches that are being counted. As a result, we can apply standard Differential Privacy techniques to the count total. 

Our Differentially Private count would result in plausible deniability of how any single individual voted. This would hold true even in the case of uniformity in the intermediate count, i.e. everyone votes the same way, as the addition of the noise on all the counts in the table would prevent an adversary determining whether there was uniformity. 

Since we are only seeking to protect counts we only need a simple form of differential privacy. Furthermore, our responses are only mutually exclusive counts of up votes or dismissals. We can assume the sensitivity to always be 1 because the maximum change that can occur in the response should an individual be included or excluded from the underlying dataset is a change in one of the totals by maximum of 1. It is therefore possible to calculate the laplacian noise independently of the underlying vote data and as a result, it can be both be generated in advance and without the need to see the decrypted data. This allows us to apply standard differential privacy in the encrypted domain, assuming we can generate suitable noise in a verifiable manner, which will be discussed further below. 

\subsection{Impact on noise on overall totals}
Clearly the addition of noise into the intermediate count will also impact on the total of up votes or dismissals a particular question has. However, this impact will become insignificant as more votes are counted and will average itself out over questions with similar number of votes. Therefore the impact on determine which question is the most or least popular should be negligible over time. Furthermore, we do not require an exact count, and most of the time exact counts will not be possible due to the batching of votes into intermediate counts, and batches for different questions not meeting the threshold at the same time. Therefore the inherent margin of error in the counting procedure is likely to exceed that introduced by Differential Privacy, which in any case will on average impact equally on all questions as they approach similar number of votes. 

%Since we are dealing with a simple counting query the sensitivity will always be 1. As such, we know what the laplace noise distribution will be in advance and don't need to perform a sensitivity analysis on the actual data.

\VTNote{Suggest writing this section to say more about what we're adding, which I agree is stock-standard DP. Then, in the next section, discuss how this can be added in a verifiably valid fashion. Occurs to me that the DP (or other) distribution may be completely independent of the actual distribution being added.}

\subsection{Verifiable Differential Privacy}
We do not want to have a trusted curator to perform the job of running the differential privacy mechanism on decrypted results as this would allow the curator to learn the unprotected result and breach the privacy of the individual voter. As a result, we want to apply the noise in the encrypted domain. As a result, we will be homomorphically adding an encrypted value containing the noise to the sum of votes prior to decryption. This presents two challenges:
\begin{itemize}
	\item How do we generate verifiable random noise?
	\item How do we prevent the generator of the random noise from being able to remove it from the decrypted result?
\end{itemize}

To achieve above two requirements we propose a simple approach for verifiably generating the laplacian noise in a manner that both audits the noise for validation construction and prevents the generator from knowing which piece of noise is being used and as a result prevents them from inverting the noise addition once the result is decrypted. 

\paragraph{Pre-Generation of Noise}
One party in advanced generates a large amount of noise from the laplace distribution and commits, encrypts, and publishes the encryptions/commitments on the bulletin board. This could be a batch in the thousands, it depends how many counts are expected, but it can always be repeated to generate more noise.

Once committed to, a random percentage of the generated noise is revealed as an audit. That percentage will depend on what probability of detection of cheating wants to be achieved, however, it should a reasonably large number, maybe 50\%. The reason a large number is needed is that once all the data has been opened it should form a distribution closely approximate to the intended laplacian distribution. If it does not then the whole batch is rejected. What counts as close will need some further thought and experimentation, if the number of values audited is large enough it should be very close to the intended laplace distribution.

The remaining unopened values should be verifiably mixed using a mix-net, potentially run by the guardians or some other distributed trust platform, to prevent the generating party knowing the mapping between the final random values and the ones it generated. The mixed encrypted random values should be published on the bulletin board.

\paragraph{Adding Noise During the Count}

The votes are summed as normal and then an encrypted noise value is selected at random from the bulletin board and also added to the sum. The selection of the noise to use could use something like the Fiat-Shamir heuristic to generate a seed for a random coin flipping function.

The combined vote and noise is then decrypted giving a differentially private count of those votes.

Possible useful background on Verifiable DP: \cite{narayan2015verifiable,kato2021preventing}.

\subsection{Are questions independent?}
Whether the votes on questions are independent or not potentially impacts on the validity of our differential privacy claims. If questions are not independent then we should accumulate the respective $\epsilon$ values. However, this would be impractical because their is no bound on how many questions a user could vote on and we cannot stop counting their votes once their privacy budget has been exhausted. We also have to consider that information outside of the counts is public and would therefore provide information that is not differentially private. For example, whether an individual had voted in some form on question would be recorded on the bulletin board in the form of their encrypted ballot. As a result, it would already be possibly to determine if an individual had a particular interest in a specific topic by virtue of how many times they had voted on similar questions. The question is therefore whether the count reveals additional information. Unprotected it certainly would, since it would be possible in cases of uniformity to determine exactly how an individual had voted. 

The uniformity risk could also be exploited by a third-party attacker. The attacker could wait for an individual to vote into a fresh batch for a question and then using $n-1$ sock-puppet or compromise accounts, cast $n-1$ votes uniformly to force a new intermediate count with only the attackers and the single target individual's vote. 

The differentially private count would prevent such an attack. Whilst the true privacy budget of the system as whole is likely to exceed the value set per count, that will be true in any case because non-differentially private data is being released. As such, it seems reasonable to treat the questions as independent and allow disjoint responses to be constructed with independent privacy budgets. 


\bibliographystyle{alpha}
\bibliography{bib}

\end{document}